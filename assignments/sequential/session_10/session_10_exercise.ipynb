{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Deep Learning - Master in Deep Learning of UPM</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "Usaremos PyTorch Lightning para unificar arquitectura y l√≥gica de entrenamiento en un √∫nico m√≥dulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sesi√≥n implementaremos _from scratch_ un Transformer utilizando √∫nicamente `PyTorch` para construir la arquitectura y `Lightning` para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 0\n",
    "        self.str2tok = {}\n",
    "        self.tok2str = {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_data(cls, data):\n",
    "        tokenizer = cls()\n",
    "\n",
    "        vocab = sorted(list(set(data)))\n",
    "        tokenizer.vocab_size = len(vocab)\n",
    "\n",
    "        tokenizer.str2tok = {c: t for t, c in enumerate(vocab)}\n",
    "        tokenizer.tok2str = {t: c for t, c in enumerate(vocab)}\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def encode(self, text):\n",
    "        _encode = lambda text: [self.str2tok[c] for c in text]\n",
    "\n",
    "        if isinstance(text, list):\n",
    "            return [_encode(s) for s in text]\n",
    "        else:\n",
    "            return _encode(text)\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return \"\".join([self.tok2str[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [EJERCICIO] TinyNextToken Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del Dataset\n",
    "\n",
    "En este bloque de c√≥digo, cargamos el conjunto de datos que utilizaremos para entrenar. Utilizamos concretamente el texto de Shakespeare, pero pod√©is probar con otros textos si lo dese√°is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/shakespeare.txt'\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "En autoregresi√≥n, el objetivo es predecir el siguiente token en una secuencia dada una serie de tokens anteriores. Por lo tanto, para cada secuencia de entrada, el objetivo (target) ser√° la misma secuencia desplazada una posici√≥n hacia la derecha.\n",
    "\n",
    "Para la secuencia de entrada:\n",
    "\n",
    "```\"The cat sat on the mat\"```\n",
    "\n",
    "La entrada correspondiente ser√°:\n",
    "\n",
    "```\"The cat sat on the\"```\n",
    "\n",
    "El objetivo correspondiente ser√°:\n",
    "\n",
    "```\"cat sat on the mat\"```\n",
    "\n",
    "De esta forma:\n",
    "- \"The\" predice \"cat\"\n",
    "- \"The cat\" predice \"sat\"\n",
    "- \"The cat sat\" predice \"on\"\n",
    "- Y as√≠ sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForAutoregression(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, seq_len=32):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len + 1 # +1 for target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx, end_idx = idx, idx+self.seq_len\n",
    "        sequence = torch.tensor(self.tokenizer.encode(self.data[start_idx:end_idx]))\n",
    "        inputs = sequence[:-1]\n",
    "        targets = sequence[1:]\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModuleForAutoregression(pytorch_lightning.LightningDataModule):\n",
    "    def __init__(self, data, tokenizer, max_seq_len=32, batch_size=64, train_size=.8, multiprocess_config={}):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "\n",
    "        self.multiprocess_config = multiprocess_config\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit':\n",
    "            dataset = DatasetForAutoregression(self.data, self.tokenizer, seq_len=self.max_seq_len)\n",
    "\n",
    "            train_size = int(len(dataset) * self.train_size)\n",
    "            val_size = len(dataset) - train_size\n",
    "            self.train_dataset, self.val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, **self.multiprocess_config)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, **self.multiprocess_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura TinyNextToken\n",
    "\n",
    "Vamos a implementar un peque√±o modelo compuesto √∫nicamente por una √∫nica capa de Embedding para predecir el siguiente token.\n",
    "\n",
    "El truco aqu√≠ es usar un `nn.Embedding` con `vocab_size` tanto para la dimensi√≥n de entrada como para la de salida. De esta forma, cada vector de embedding puede ser interpretado como un *logit* para cada token en el vocabulario.\n",
    "\n",
    "Tokens [batch_size, seq_len] -> Embedding [batch_size, seq_len, vocab_size] -> Predecimos el siguiente token.\n",
    "\n",
    "La funci√≥n de p√©rdida ser√° una `CrossEntropyLoss` est√°ndar, pero ¬øc√≥mo funciona la autorregresi√≥n en este caso?\n",
    "\n",
    "1. Durante el entrenamiento, para cada secuencia de entrada, desplazamos los tokens de salida una posici√≥n a la izquierda. Esto significa que el modelo aprende a predecir el token siguiente en la secuencia.\n",
    "    - Input: [The cat sat on the]\n",
    "    - Target: [cat sat on the mat]\n",
    "\n",
    "2. La funci√≥n de p√©rdida compara las predicciones del modelo con los tokens objetivo desplazados, calculando la p√©rdida solo en las posiciones correspondientes a los tokens reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class TinyNextTokenModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        lr=1e-3,\n",
    "        max_seq_len=32,\n",
    "        pad_token=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size) # Embedding layer\n",
    "        self.criterion = nn.CrossEntropyLoss() # Classification loss for autoregression\n",
    "\n",
    "        self.lr = lr # Learning rate\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        return x\n",
    "\n",
    "    def compute_loss(self, batch, split='train'):\n",
    "        inputs, targets = batch\n",
    "        logits = self(inputs)\n",
    "        \n",
    "        B, T, C = logits.size()\n",
    "        logits = logits.reshape(B * T, C)\n",
    "        targets = targets.reshape(B * T)\n",
    "\n",
    "        loss = self.criterion(logits, targets)\n",
    "\n",
    "        self.log(\n",
    "            f'{split}_loss',\n",
    "            loss,\n",
    "            on_step=(split=='train'),\n",
    "            on_epoch=True,\n",
    "            prog_bar=(split=='train')\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.compute_loss(batch, split='train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.compute_loss(batch, split='val')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "tokenizer = CharTokenizer.from_data(data)\n",
    "max_seq_len = 128\n",
    "batch_size = 32\n",
    "train_size = 0.8\n",
    "learning_rate = 1e-3\n",
    "training_steps = 5_000\n",
    "\n",
    "# Create DataModule\n",
    "data_module = DataModuleForAutoregression(\n",
    "    data=data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=max_seq_len,        # puedes cambiarlo\n",
    "    batch_size=batch_size,          # bastante seguro\n",
    "    train_size=train_size,\n",
    "    multiprocess_config={'num_workers': 2}\n",
    ")\n",
    "\n",
    "module = TinyNextTokenModule(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    lr=learning_rate,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_steps=training_steps,              \n",
    "    accelerator=\"gpu\",\n",
    "    devices=[5],\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos una funci√≥n que nos permita generar texto. Funcionar√° de la siguiente manera:\n",
    "- Dada una secuencia inicial (text), el modelo predice el siguiente token. El _cual_ elige se conoce como sampling y es algo que veremos m√°s adelante.\n",
    "- Ese token se a√±ade a la secuencia.\n",
    "- El proceso se repite hasta alcanzar la longitud deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(model, tokenizer, text, max_new_tokens=100, temperature=1.0, top_k=40):\n",
    "    \"\"\"\n",
    "    Autoregressive text generation with top-k sampling and temperature.\n",
    "    - model: trained TinyNextTokenModule (or compatible)\n",
    "    - tokenizer: tokenizer with encode/decode methods\n",
    "    - text: initial prompt string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encode prompt ‚Üí tensor shape: (1, seq_len)\n",
    "    input_ids = torch.tensor([tokenizer.encode(text)], dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Forward pass ‚Üí (1, seq_len, vocab_size)\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "\n",
    "        # Select last timestep logits ‚Üí (vocab_size,)\n",
    "        logits = logits[0, -1]\n",
    "\n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Optional top-k filtering\n",
    "        if top_k is not None:\n",
    "            values, indices = torch.topk(logits, top_k)\n",
    "            mask = torch.full_like(logits, float('-inf'))\n",
    "            mask[indices] = logits[indices]\n",
    "            logits = mask\n",
    "\n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample from distribution\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        # Append token ‚Üí new shape (1, seq_len+1)\n",
    "        next_token_tensor = torch.tensor([[next_token]], device=device)\n",
    "        input_ids = torch.cat([input_ids, next_token_tensor], dim=1)\n",
    "\n",
    "    # Decode entire sequence (move to CPU first)\n",
    "    return tokenizer.decode(input_ids[0].cpu().tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que previo a ser entrenado genera texto sin ning√∫n sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To be, or not to be, that is theKS\\n.,bcnxVNWqV;F-&KI?;I'uMEgRHERNcR?!AroNyYc&dx'YxVg.a-$eDE,niE3NLdS\\ndwhAUmHE.iNpk!PclE,tr.rQyRuArq\\n\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(generate(module, tokenizer, text=\"To be, or not to be, that is the\", max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenemos esperando que al menos aprenda cierta _forma_ del corpus. (Puede llevar un par de minutos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\n",
      "\n",
      "  | Name      | Type             | Params | Mode\n",
      "------------------------------------------------------\n",
      "0 | embedding | Embedding        | 4.2 K  | eval\n",
      "1 | criterion | CrossEntropyLoss | 0      | eval\n",
      "------------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "2         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a325c52f5f4d45dfac23d21853299531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/synthetic-generation/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:527: Found 3 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894e07dadcf344bb99b7ae6bab07886f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, or not to be, that is the n h amajur nl! sls wous, tory ton?\n",
      "Whass t:\n",
      "S:\n",
      "A llly won lewouth\n",
      "T:\n",
      "Bure pr nge m CENG ng pof po g\n"
     ]
    }
   ],
   "source": [
    "print(generate(module, tokenizer, text=\"To be, or not to be, that is the\", max_new_tokens=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo es extremadamente simple y nos sirve como baseline para adentrarnos en el mundo de los Transformers. Aqu√≠ los tokens no est√°n _hablando entre ellos_ ya que no hay intercambio de informaci√≥n entre posiciones. Realmente para predecir el siguiente token, el modelo solo puede basarse en la informaci√≥n del token actual, concretamente en el embedding para ese token espec√≠fico, que adem√°s hace de logit. Esto quiere decir que cada embedding en este caso va a intentar maximizar la probabilidad del token que le sigue en el dataset.\n",
    "\n",
    "El embedding correspondiente a cada token ser√° una especie de media ponderada de los tokens que le siguen en el dataset. Esto por supuesto no es suficiente para generar texto coherente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthetic-generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
