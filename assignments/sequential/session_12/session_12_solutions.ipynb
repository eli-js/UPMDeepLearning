{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Deep Learning - Master in Deep Learning of UPM</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "Usaremos PyTorch Lightning para unificar arquitectura y lógica de entrenamiento en un único módulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sesión implementaremos _from scratch_ un Transformer utilizando únicamente `PyTorch` para construir la arquitectura y `Lightning` para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manejo de datos y Tokenizado\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del Dataset\n",
    "\n",
    "En este bloque de código, cargamos el conjunto de datos que utilizaremos para entrenar. Podéis elegir entre tres corpus: *Shakespeare*, *Rick and Morty* o *Tarantino*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/shakespeare.txt'\n",
    "# data_path = 'data/rick_and_morty.txt'\n",
    "# data_path = 'data/tarantino.txt'\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 0\n",
    "        self.str2tok = {}\n",
    "        self.tok2str = {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_data(cls, data, add_pad_token=True):\n",
    "        tokenizer = cls()\n",
    "\n",
    "        vocab = sorted(list(set(data)))\n",
    "        if add_pad_token:\n",
    "            vocab.insert(0, '<pad>')\n",
    "\n",
    "        tokenizer.vocab_size = len(vocab)\n",
    "\n",
    "        tokenizer.str2tok = {c: t for t, c in enumerate(vocab)}\n",
    "        tokenizer.tok2str = {t: c for t, c in enumerate(vocab)}\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def encode(self, text):\n",
    "        _encode = lambda text: [self.str2tok[c] for c in text]\n",
    "\n",
    "        if isinstance(text, list):\n",
    "            return [_encode(s) for s in text]\n",
    "        else:\n",
    "            return _encode(text)\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return ''.join([self.tok2str[t] for t in tokens])\n",
    "    \n",
    "    @property\n",
    "    def pad_token(self):\n",
    "        return self.str2tok['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDatasetForAutoregression(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, seq_len=512, windowed=False):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len + 1 # +1 for target\n",
    "        self.windowed = windowed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len + 1 if not self.windowed else len(self.data) // self.seq_len + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx, end_idx = (idx, idx+self.seq_len) if not self.windowed else (idx*self.seq_len,idx*self.seq_len+self.seq_len)\n",
    "\n",
    "        input_ids = self.tokenizer.encode(self.data[start_idx:end_idx])\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, pad_token=0, pad_site='right', return_mask=False):\n",
    "    max_seq_len = max([len(seq) for seq in sequences])\n",
    "\n",
    "    def pad_sequence(seq):\n",
    "        pad_len = (max_seq_len - len(seq))\n",
    "        pad = [pad_token] * pad_len\n",
    "        if pad_site == 'right':\n",
    "            pad_seq = seq + pad\n",
    "            pad_mask = [1] * len(seq) + [0] * pad_len\n",
    "            return pad_seq, pad_mask\n",
    "        elif pad_site == 'left':\n",
    "            pad_seq = pad + seq\n",
    "            pad_mask = [0] * pad_len + [1] * len(seq)\n",
    "            return pad_seq, pad_mask\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid padding site: {pad_site}. Consider using 'right' or 'left'.\")\n",
    "    \n",
    "    pad_seqs, pad_masks = [], []\n",
    "    for seq in sequences:\n",
    "        pad_seq, pad_mask = pad_sequence(seq)\n",
    "        pad_seqs.append(pad_seq)\n",
    "        pad_masks.append(pad_mask)\n",
    "\n",
    "    return (pad_seqs, pad_masks) if return_mask else pad_seqs\n",
    "    \n",
    "class CollatorForAutoregression:\n",
    "    def __init__(self, pad_token=0):\n",
    "        self.pad_token = pad_token\n",
    "    \n",
    "    def __call__(self, sequences):\n",
    "        context, mask = pad_sequences(sequences, pad_token=self.pad_token, return_mask=True)\n",
    "\n",
    "        context, mask = torch.tensor(context), torch.tensor(mask)\n",
    "\n",
    "        input_ids, att_mask = context[:, :-1], mask[:, :-1] # [:seq_len-1] -> batch_size x seq_len\n",
    "        target_ids, target_att_mask = context[:, 1:], mask[:, 1:] # [1:seq_len] -> batch_size x seq_len\n",
    "\n",
    "        return input_ids, att_mask, target_ids, target_att_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataModuleForAutoregression(pytorch_lightning.LightningDataModule):\n",
    "    def __init__(self, data, max_seq_len=512, batch_size=64, train_size=.8, windowed=False, multiprocess_config={}):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = CharTokenizer.from_data(data)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.windowed = windowed\n",
    "\n",
    "        self.multiprocess_config = multiprocess_config\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit':\n",
    "            dataset = CharDatasetForAutoregression(self.data, self.tokenizer, seq_len=self.max_seq_len, windowed=self.windowed)\n",
    "\n",
    "            train_size = int(len(dataset) * self.train_size)\n",
    "            val_size = len(dataset) - train_size\n",
    "            self.train_dataset, self.val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "            self.collate_fn = CollatorForAutoregression(pad_token=self.tokenizer.pad_token)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, shuffle=True, **self.multiprocess_config)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, **self.multiprocess_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Posicional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La codificación sinusoidal permite que el modelo entienda relaciones relativas de manera eficiente, sin importar la longitud de la oración. Esto es clave porque el lenguaje se basa en patrones de relación más que en posiciones absolutas. Si un Transformer aprende que **el verbo suele estar después del sujeto, esta relación se mantendrá sin importar la posición exacta en la oración.** \n",
    "\n",
    "Si tomamos la ecuación del Positional Encoding:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "Podemos ver que si queremos calcular el encoding de una posición desplazada \\( pos+k \\):\n",
    "\n",
    "$$\n",
    "PE(pos + k, 2i) = \\sin\\left(\\frac{pos + k}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos + k, 2i+1) = \\cos\\left(\\frac{pos + k}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "Usando propiedades trigonométricas como:\n",
    "\n",
    "$$\n",
    "\\sin(A + B) = \\sin A \\cos B + \\cos A \\sin B\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\cos(A + B) = \\cos A \\cos B - \\sin A \\sin B\n",
    "$$\n",
    "\n",
    "Esto nos permite expresar $ PE_{pos+k} $ **como una combinación lineal de $ PE_{pos} $**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model, freq=10_000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "        pos = torch.arange(0, max_seq_len).unsqueeze(-1) # (max_seq_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(freq)) / d_model)) # e^(2i * -(log(freq) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term) # PE(pos, 2i) = sin(pos / freq^(2i/d_model))\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term) # PE(pos, 2i+1) = cos(pos / freq^(2i/d_model))\n",
    "\n",
    "        pe = pe.unsqueeze(0) # add batch dimension\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Parámetros iniciales\n",
    "max_seq_len = 100  # Número de posiciones en la oración\n",
    "d_model = 16       # Dimensiones del embedding\n",
    "freq = 10_000  # Frecuencia base\n",
    "\n",
    "# Función para graficar interactivamente\n",
    "def plot_positional_encoding(d=0, freq=10_000):\n",
    "    pos = np.arange(max_seq_len) # seq_len\n",
    "    pe = SinusoidalPositionalEncoding(max_seq_len, d_model, freq=freq).pe.squeeze(0).numpy() # seq_len x d_model\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "\n",
    "    # Graficar la dimensión seleccionada\n",
    "    plt.plot(pos, pe[:, d], label=f'Sin d={d}', color='b') # sin(pos / freq^(2i/d_model))\n",
    "    plt.plot(pos, pe[:, d+1], linestyle=\"dashed\", label=f'Cos d={d+1}', color='r') # cos(pos / freq^(2i/d_model))\n",
    "    \n",
    "    plt.xlabel(\"Posición en la secuencia\")\n",
    "    plt.ylabel(\"Valor de Encoding\")\n",
    "    plt.title(f\"Positional Encoding para dimensión {d} y {d+1}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Interfaz interactiva con sliders\n",
    "interact(plot_positional_encoding, \n",
    "         d=widgets.IntSlider(min=0, max=d_model-1, step=2, value=0, description=\"Dimensión\"),\n",
    "         freq=widgets.IntSlider(min=1000, max=20000, step=1000, value=freq, description=\"Frecuencia\"));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Parámetros iniciales\n",
    "max_seq_len = 512  # Número de posiciones en la oración\n",
    "d_model = 512      # Dimensiones del embedding\n",
    "freq = 10_000  # Frecuencia base\n",
    "\n",
    "\n",
    "pos = np.arange(max_seq_len) # seq_len\n",
    "pe = SinusoidalPositionalEncoding(max_seq_len, d_model, freq=freq).pe.squeeze(0).numpy() # seq_len x d_model\n",
    "\n",
    "# Graficar Heatmap\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(pe, cmap=\"coolwarm\", xticklabels=50, yticklabels=50)\n",
    "plt.xlabel(\"Dimensión del Embedding\")\n",
    "plt.ylabel(\"Posición en la Secuencia\")\n",
    "plt.title(\"Mapa de Calor del Positional Encoding\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente bloque de código se puede apreciar que si escalamos los valores, la función softmax tiende a la función de one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3], [2, 3, 6]], dtype=torch.float32)\n",
    "\n",
    "print(torch.softmax(x, dim=-1))\n",
    "\n",
    "scale_by = 100\n",
    "\n",
    "print(torch.softmax(x * scale_by, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creemos nuestra propia función softmax y comprobemos que funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, dim=-1):\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x), dim=dim, keepdim=True)\n",
    "\n",
    "x = torch.randn(2, 3)\n",
    "assert torch.allclose(softmax(x), torch.softmax(x, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación creemos nuesta propia _scaeled dot product attention_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, att_mask=None, is_causal=False, p_drop=0.0):\n",
    "    seq_len, d_k = k.size(-2), k.size(-1) # sequence length, head dimension\n",
    "    scale_factor = d_k ** -.5 # scale factor for softmax\n",
    "    att_scores = q @ k.transpose(-2, -1) * scale_factor # batch_size x num_heads x seq_len x seq_len\n",
    "\n",
    "    if is_causal:\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=att_scores.device))\n",
    "        att_scores = att_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "    \n",
    "    if att_mask is not None:\n",
    "        att_scores = att_scores.masked_fill(att_mask.unsqueeze(1).unsqueeze(2) == 0, float('-inf'))\n",
    "\n",
    "    att_scores = softmax(att_scores, dim=-1)\n",
    "    att_scores = torch.nn.functional.dropout(att_scores, p=p_drop)\n",
    "    return att_scores @ v # seq_len x d_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = d_model // num_heads\n",
    "\n",
    "        self.q_proj = torch.nn.Linear(d_model, d_model)\n",
    "        self.k_proj = torch.nn.Linear(d_model, d_model)\n",
    "        self.v_proj = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out_proj = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, att_mask=None, is_causal=False):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        q = self.q_proj(x) # batch_size x seq_len x d_model\n",
    "        k = self.k_proj(x) # batch_size x seq_len x d_model\n",
    "        v = self.v_proj(x) # batch_size x seq_len x d_model\n",
    "\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_size) # batch_size x seq_len x num_heads x head_size\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_size) # batch_size x seq_len x num_heads x head_size\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_size) # batch_size x seq_len x num_heads x head_size\n",
    "\n",
    "        q = q.transpose(1, 2) # batch_size x num_heads x seq_len x head_size\n",
    "        k = k.transpose(1, 2) # batch_size x num_heads x seq_len x head_size\n",
    "        v = v.transpose(1, 2) # batch_size x num_heads x seq_len x head_size\n",
    "\n",
    "        att = scaled_dot_product_attention(q, k, v, att_mask=att_mask, is_causal=is_causal) # batch_size x num_heads x seq_len x head_size\n",
    "        att = att.transpose(1, 2) # batch_size x seq_len x num_heads x head_size\n",
    "        att = att.reshape(batch_size, seq_len, self.num_heads * self.head_size) # batch_size x seq_len x d_model\n",
    "        return self.out_proj(att) # batch_size x seq_len x d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-Wise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = torch.nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x) # batch_size x seq_len x d_ff\n",
    "        x = self.act(x) # batch_size x seq_len x d_ff\n",
    "        x = self.linear_2(x) # batch_size x seq_len x d_model\n",
    "        return x # batch_size x seq_len x d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capa Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, p_drop=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.pwff = PositionWiseFFN(d_model, d_ff)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "        self.layer_norm_mhsa = torch.nn.LayerNorm(d_model)\n",
    "        self.layer_norm_pwff = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, att_mask=None, is_causal=False):\n",
    "        mhsa_out = self.mhsa(x, att_mask=att_mask, is_causal=is_causal) # batch_size x seq_len x d_model\n",
    "        x = self.layer_norm_mhsa(x + self.dropout(mhsa_out)) # batch_size x seq_len x d_model\n",
    "\n",
    "        pwff_out = self.pwff(x) # batch_size x seq_len x d_model\n",
    "        x = self.layer_norm_pwff(x + self.dropout(pwff_out)) # batch_size x seq_len x d_model\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model        : int,          # transformer hidden dimension\n",
    "                 num_heads      : int,          # number of heads in multi-head attention\n",
    "                 num_layers     : int,          # number of transformer layers\n",
    "                 d_ff           : int,          # pos-wise feed-forward hidden dimension\n",
    "                 p_drop         : float = 0.1,  # dropout probability\n",
    "                 is_causal      : bool = False  # causal attention mask\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.transformer_layers = torch.nn.ModuleList([TransformerLayer(d_model, num_heads, d_ff, p_drop) for _ in range(num_layers)])\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "    def forward(self, x, att_mask=None):\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x, att_mask=att_mask, is_causal=self.is_causal)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder y Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderOnly(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model        : int,                                  # transformer hidden dimension\n",
    "                 num_heads      : int,                                  # number of heads in multi-head attention\n",
    "                 num_layers     : int,                                  # number of transformer layers\n",
    "                 d_ff           : int,                                  # pos-wise feed-forward hidden dimension\n",
    "                 max_seq_len    : int,                                  # maximum sequence length\n",
    "                 vocab_size     : int,                                  # vocabulary size\n",
    "                 p_drop         : float = 0.1,                          # dropout probability\n",
    "                 pos_enc                = SinusoidalPositionalEncoding  # positional encoding type\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = torch.nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = pos_enc(max_seq_len, d_model)\n",
    "\n",
    "        self.body = Transformer(d_model, num_heads, num_layers, d_ff, p_drop)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, input_ids, att_mask=None):\n",
    "        x = self.dropout(self.pos_enc(self.emb(input_ids)))\n",
    "        return self.body(x, att_mask=att_mask)\n",
    "    \n",
    "class TransformerDecoderOnly(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model        : int,                                  # transformer hidden dimension\n",
    "                 num_heads      : int,                                  # number of heads in multi-head attention\n",
    "                 num_layers     : int,                                  # number of transformer layers\n",
    "                 d_ff           : int,                                  # pos-wise feed-forward hidden dimension\n",
    "                 max_seq_len    : int,                                  # maximum sequence length\n",
    "                 vocab_size     : int,                                  # vocabulary size\n",
    "                 p_drop         : float = 0.1,                          # dropout probability\n",
    "                 pos_enc                = SinusoidalPositionalEncoding  # positional encoding type\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = torch.nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = pos_enc(max_seq_len, d_model)\n",
    "\n",
    "        self.transformer = Transformer(d_model, num_heads, num_layers, d_ff, p_drop, is_causal=True)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, input_ids, att_mask=None):\n",
    "        x = self.dropout(self.pos_enc(self.emb(input_ids)))\n",
    "        return self.transformer(x, att_mask=att_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Para Modelado Causal del Lenguaje (Autorregresión)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForCausalLM(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model        : int,                                  # transformer hidden dimension\n",
    "                 num_heads      : int,                                  # number of heads in multi-head attention\n",
    "                 num_layers     : int,                                  # number of transformer layers\n",
    "                 d_ff           : int,                                  # pos-wise feed-forward hidden dimension\n",
    "                 max_seq_len    : int,                                  # maximum sequence length\n",
    "                 vocab_size     : int,                                  # vocabulary size\n",
    "                 p_drop         : float = 0.1,                          # dropout probability\n",
    "                 pos_enc                = SinusoidalPositionalEncoding  # positional encoding type\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.body = TransformerDecoderOnly(d_model, num_heads, num_layers, d_ff, max_seq_len, vocab_size, p_drop, pos_enc)\n",
    "        self.head = torch.nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, att_mask=None):\n",
    "        x = self.body(input_ids, att_mask=att_mask)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Módulo Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLMModule(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, \n",
    "                 d_model        : int,                                  # transformer hidden dimension\n",
    "                 num_heads      : int,                                  # number of heads in multi-head attention\n",
    "                 num_layers     : int,                                  # number of transformer layers\n",
    "                 d_ff           : int,                                  # pos-wise feed-forward hidden dimension\n",
    "                 max_seq_len    : int,                                  # maximum sequence length\n",
    "                 vocab_size     : int,                                  # vocabulary size\n",
    "                 p_drop         : float = 0.1,                          # dropout probability\n",
    "                 pos_enc                = SinusoidalPositionalEncoding, # positional encoding type\n",
    "                 optim_params   : dict  = {'lr': 1e-4}                  # optimizer parameters\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.optim_params = optim_params\n",
    "        self.model = TransformerForCausalLM(d_model, num_heads, num_layers, d_ff, max_seq_len, vocab_size, p_drop, pos_enc)\n",
    "\n",
    "    def forward(self, input_ids, att_mask=None):\n",
    "        return self.model(input_ids, att_mask)\n",
    "    \n",
    "    def _step(self, batch):\n",
    "        input_ids, att_mask, target_ids, target_att_mask = batch\n",
    "        logits = self(input_ids, att_mask)\n",
    "        \n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "        logits = logits.view(batch_size * seq_len, vocab_size) # (batch_size * seq_len) x vocab_size\n",
    "        target_ids = target_ids.reshape(batch_size * seq_len) # (batch_size * seq_len)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, target_ids, reduction='none') # (batch_size * seq_len)\n",
    "        target_att_mask = target_att_mask.view(-1) / target_att_mask.sum() # (batch_size * seq_len) normalized for averaging\n",
    "        return (loss * target_att_mask).sum() # sum up the loss and normalize by the number of non-padding tokens\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), **self.optim_params)\n",
    "    \n",
    "    def configure_callbacks(self):\n",
    "        return super().configure_callbacks() + [pytorch_lightning.callbacks.ModelCheckpoint(monitor='val_loss', mode='min')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformerForCausalLM params\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 8\n",
    "d_ff = 512\n",
    "\n",
    "# DataModule params\n",
    "batch_size = 64\n",
    "max_seq_len = 256\n",
    "train_size = .8\n",
    "multiprocess_config = dict(num_workers=16, prefetch_factor=4, pin_memory=True)\n",
    "windowed = True\n",
    "\n",
    "# Training params\n",
    "optimizer_params = dict(lr=3e-4, weight_decay=1e-2)\n",
    "max_epochs = 50\n",
    "max_steps = 100_000\n",
    "val_check_interval = 100\n",
    "\n",
    "data_module = CharDataModuleForAutoregression(data, max_seq_len=max_seq_len, train_size=train_size, batch_size=batch_size, windowed=windowed, multiprocess_config=multiprocess_config)\n",
    "module = CausalLMModule(d_model=d_model, num_heads=num_heads, num_layers=num_layers, d_ff=d_ff, max_seq_len=max_seq_len, vocab_size=data_module.tokenizer.vocab_size, optim_params=optimizer_params)\n",
    "trainer = pytorch_lightning.Trainer(max_epochs=max_epochs, devices=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling e Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(logits):\n",
    "    # logits: (batch_size, vocab_size)\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "def sampling_decoding(logits):\n",
    "    # logits: (batch_size, vocab_size)\n",
    "    return torch.multinomial(torch.nn.functional.softmax(logits, dim=-1), num_samples=1)\n",
    "\n",
    "def complete_text(\n",
    "    decoding_fn,\n",
    "    text,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    num_completions=3,\n",
    "    max_len=128,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    for _ in range(num_completions):\n",
    "        encoded_text = tokenizer.encode(text)[-max_len:]  # NOTE: Truncate the text! The model was\n",
    "                                                          # trained with a maximum length, and will \n",
    "                                                          # break down if the input is longer than that.\n",
    "                                                          \n",
    "        encoded_text = torch.tensor(encoded_text).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor([[1] * len(encoded_text[0])]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(encoded_text, attention_mask)  # (1, seq_len, vocab_size)\n",
    "            output = output[:, -1, :]  # (1, vocab_size)\n",
    "\n",
    "            next_tokens = decoding_fn(output)  # (1, 1)\n",
    "\n",
    "            predicted_char = tokenizer.tok2str[next_tokens.item()]\n",
    "\n",
    "        text += predicted_char\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = 'lightning_logs/version_0/checkpoints/epoch=49-step=49999.ckpt'\n",
    "\n",
    "module = CausalLMModule.load_from_checkpoint(MODEL_CKPT)\n",
    "module.eval()\n",
    "\n",
    "greedy_decoding_fn = lambda x: greedy_decoding(x)\n",
    "sampling_decoding_fn = lambda x: sampling_decoding(x)\n",
    "\n",
    "text = \"To be or not to be, that is the question:\"\n",
    "\n",
    "greedy_text = complete_text(greedy_decoding_fn, text, data_module.tokenizer, module, num_completions=1000, max_seq_len=max_seq_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
