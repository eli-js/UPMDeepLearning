{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Deep Learning - Master in Deep Learning of UPM</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "Antes de empezar debemos instalar PyTorch Lightning, por defecto, esto valdría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, si te encuentras ejecutando este código en Google Collab, lo mejor será que montes tu drive para tener acceso a los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio práctico vamos a utilizar los conocimientos adquiridos para abordar un caso de Regresión mediante PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/exercise.csv' # Pon tu ruta dependiendo de donde tengas el archivo en el Drive\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head() # Imprimamos las primeras filas del dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "class RegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        ...\n",
    "\n",
    "    def __len__(self):\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ...\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NO TOCAR\n",
    "def split_train_val_test(df, val_size=0.2, test_size=0.2):\n",
    "    eval_size = val_size + test_size # eval es un split intermedio que luego se divide en val y test\n",
    "    test_prop = test_size / eval_size # proporción de test respecto a eval\n",
    "\n",
    "    train, eval_ = train_test_split(df, test_size=eval_size)\n",
    "    val, test = train_test_split(eval_, test_size=test_prop)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El DataModule debe recibir dinámicamente:\n",
    "- El batch_size, por defecto a 64\n",
    "- El número de trabajadores (num_workers), por defecto a 4\n",
    "- El prefetch_factor, por defecto a 2\n",
    "- Y el pin_memory, por defecto a True\n",
    "\n",
    "Con ello deberá inicializar los dataloaders para entrenamiento, validación y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class RegressionDataModule(pytorch_lightning.LightningDataModule):\n",
    "    def __init__(self, df, ...):\n",
    "        super().__init__()\n",
    "        ...\n",
    "\n",
    "    def setup(self, stage=None): # esta función la ejecuta el trainer cuando se va a ejecutar el fit o el predict\n",
    "        ...\n",
    "\n",
    "    def collate_fn(self, batch): # PISTA: recordad que esto es regresión...\n",
    "        ...\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        ...\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        ...\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightningModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red debe ser un MLP a vuestra elección, también el optimizador.\n",
    "\n",
    "Sin embargo, la función de pérdida se deberá adecuar para el caso de regresión...\n",
    "\n",
    "La métrica que se deberá guardar será el R2 (disponible en [torchmetrics](https://lightning.ai/docs/torchmetrics/stable/regression/r2_score.html))\n",
    "\n",
    "Ya os habréis fijado en que los steps de entrenamiento y validación son bastante semejante. Para ahorrar código, vamos a abstraer la parte común a otra función compute_batch() que recibirá el batch y el split para el que se realiza el step. Esta es la magia de Lightning, esta función no es nativa del LightningModule, es cosecha propia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import R2Score\n",
    "import torch.nn as nn\n",
    "\n",
    "class Regressor(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        # Inicializamos las capas de la red\n",
    "        ...\n",
    "\n",
    "        # Función de pérdida\n",
    "        self.criterion = ...\n",
    "\n",
    "        # Inicializamos las métricas\n",
    "        self.r2 = R2Score()\n",
    "\n",
    "    # Función forward como en un nn.Module de PyTorch\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def compute_batch(self, batch, split='train'):\n",
    "        ...\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'val')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.compute_batch(batch, 'test')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks, Loggers y Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio extra** - Implementar un callback que haga de timer. Este tendrá que registrar el momento en el que empieza el entrenamiento e imprimir el tiempo que ha transcurrido cuando este termina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning\n",
    "import time\n",
    "\n",
    "class Timer(pytorch_lightning.Callback):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "SAVE_DIR = f'lightning_logs/exercise/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "\n",
    "\n",
    "# DataModule (NO TOCAR)\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data_module = RegressionDataModule(data, batch_size=64)\n",
    "\n",
    "# LightningModule\n",
    "input_shape = ... # Encontrar una manera dinámica de obtener el input_shape, PISTA: df.shape\n",
    "model = Regressor(input_shape=input_shape)\n",
    "\n",
    "# Callbacks\n",
    "# Se deberá monitorizar la métrica 'val_r2' y guardar únicamente el mejor modelo.\n",
    "early_stopping_callback = pytorch_lightning.callbacks.EarlyStopping(\n",
    "    ...\n",
    ")\n",
    "model_checkpoint_callback = pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "    ...\n",
    ")\n",
    "\n",
    "# Descomentar las dos líneas siguientes si se ha implementado el callback Timer\n",
    "# timer_callback = Timer()\n",
    "# callbacks = [early_stopping_callback, model_checkpoint_callback, timer_callback]\n",
    "\n",
    "callbacks = [early_stopping_callback, model_checkpoint_callback]\n",
    "\n",
    "# Loggers (NO TOCAR)\n",
    "csv_logger = pytorch_lightning.loggers.CSVLogger(\n",
    "    save_dir=SAVE_DIR,\n",
    "    name='metrics',\n",
    "    version=None\n",
    ")\n",
    "\n",
    "loggers = [csv_logger] # se pueden poner varios loggers (mirar documentación)\n",
    "\n",
    "# Trainer (NO TOCAR)\n",
    "trainer = pytorch_lightning.Trainer(max_epochs=50, accelerator='gpu', devices=[0], callbacks=callbacks, logger=loggers)\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "results = trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = data_module.test_df.sample(10)\n",
    "\n",
    "inputs = torch.tensor(test_sample.drop('target', axis=1).values, dtype=torch.float32)\n",
    "targets = torch.tensor(test_sample['target'].values, dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    preds = outputs.squeeze().numpy()\n",
    "\n",
    "for i, (pred, target) in enumerate(zip(preds, targets)):\n",
    "    print(f\"Predicción {i}: {pred:.2f}, Valor real: {target:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthetic-generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
